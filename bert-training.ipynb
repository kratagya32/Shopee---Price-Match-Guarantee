{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "streaming-equivalent",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:27.315527Z",
     "iopub.status.busy": "2021-05-08T18:37:27.313541Z",
     "iopub.status.idle": "2021-05-08T18:37:34.618089Z",
     "shell.execute_reply": "2021-05-08T18:37:34.617386Z"
    },
    "papermill": {
     "duration": 7.319095,
     "end_time": "2021-05-08T18:37:34.618289",
     "exception": false,
     "start_time": "2021-05-08T18:37:27.299194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "copyfile(src = \"../input/tokenization/tokenization.py\", dst = \"../working/tokenization.py\")\n",
    "import tokenization\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clean-blood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.639382Z",
     "iopub.status.busy": "2021-05-08T18:37:34.638762Z",
     "iopub.status.idle": "2021-05-08T18:37:34.643811Z",
     "shell.execute_reply": "2021-05-08T18:37:34.643182Z"
    },
    "papermill": {
     "duration": 0.017149,
     "end_time": "2021-05-08T18:37:34.643951",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.626802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "# Seed\n",
    "SEED = 123\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "LR = 0.00001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-arrow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.664855Z",
     "iopub.status.busy": "2021-05-08T18:37:34.664164Z",
     "iopub.status.idle": "2021-05-08T18:37:34.668705Z",
     "shell.execute_reply": "2021-05-08T18:37:34.669196Z"
    },
    "papermill": {
     "duration": 0.017632,
     "end_time": "2021-05-08T18:37:34.669373",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.651741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallen-convert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.694567Z",
     "iopub.status.busy": "2021-05-08T18:37:34.693854Z",
     "iopub.status.idle": "2021-05-08T18:37:34.698273Z",
     "shell.execute_reply": "2021-05-08T18:37:34.698847Z"
    },
    "papermill": {
     "duration": 0.021097,
     "end_time": "2021-05-08T18:37:34.699022",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.677925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_and_preprocess():\n",
    "    df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "    df['matches'] = df['label_group'].map(tmp)\n",
    "    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "    encoder = LabelEncoder()\n",
    "    df['label_group'] = encoder.fit_transform(df['label_group'])\n",
    "    N_CLASSES = df['label_group'].nunique()\n",
    "    print(f'We have {N_CLASSES} classes')\n",
    "    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n",
    "    return df, N_CLASSES, x_train, x_val, y_train, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lucky-gregory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.725628Z",
     "iopub.status.busy": "2021-05-08T18:37:34.723351Z",
     "iopub.status.idle": "2021-05-08T18:37:34.726521Z",
     "shell.execute_reply": "2021-05-08T18:37:34.727120Z"
    },
    "papermill": {
     "duration": 0.020107,
     "end_time": "2021-05-08T18:37:34.727304",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.707197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return tokens, masks and segments from a text array or series\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessible-resort",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.761229Z",
     "iopub.status.busy": "2021-05-08T18:37:34.760125Z",
     "iopub.status.idle": "2021-05-08T18:37:34.763715Z",
     "shell.execute_reply": "2021-05-08T18:37:34.763147Z"
    },
    "papermill": {
     "duration": 0.028094,
     "end_time": "2021-05-08T18:37:34.763878",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.735784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "curious-number",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.792635Z",
     "iopub.status.busy": "2021-05-08T18:37:34.791514Z",
     "iopub.status.idle": "2021-05-08T18:37:34.794744Z",
     "shell.execute_reply": "2021-05-08T18:37:34.794180Z"
    },
    "papermill": {
     "duration": 0.0221,
     "end_time": "2021-05-08T18:37:34.794888",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.772788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build bert model\n",
    "def build_bert_model(bert_layer, max_len = 512):\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "    \n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    x = margin([clf_output, label])\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
    "                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interesting-madness",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-08T18:37:34.825817Z",
     "iopub.status.busy": "2021-05-08T18:37:34.825096Z",
     "iopub.status.idle": "2021-05-08T23:27:15.518769Z",
     "shell.execute_reply": "2021-05-08T23:27:15.517905Z"
    },
    "papermill": {
     "duration": 17380.715377,
     "end_time": "2021-05-08T23:27:15.518969",
     "exception": false,
     "start_time": "2021-05-08T18:37:34.803592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 11014 classes\n",
      "Epoch 1/25\n",
      "718/718 [==============================] - 736s 963ms/step - loss: 23.9380 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.3447 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 23.34470, saving model to Bert_123.h5\n",
      "Epoch 2/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 22.5654 - sparse_categorical_accuracy: 5.3551e-04 - val_loss: 22.1750 - val_sparse_categorical_accuracy: 0.0048\n",
      "\n",
      "Epoch 00002: val_loss improved from 23.34470 to 22.17502, saving model to Bert_123.h5\n",
      "Epoch 3/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 20.6327 - sparse_categorical_accuracy: 0.0053 - val_loss: 21.1433 - val_sparse_categorical_accuracy: 0.0124\n",
      "\n",
      "Epoch 00003: val_loss improved from 22.17502 to 21.14326, saving model to Bert_123.h5\n",
      "Epoch 4/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 18.5772 - sparse_categorical_accuracy: 0.0186 - val_loss: 20.2375 - val_sparse_categorical_accuracy: 0.0250\n",
      "\n",
      "Epoch 00004: val_loss improved from 21.14326 to 20.23747, saving model to Bert_123.h5\n",
      "Epoch 5/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 16.5940 - sparse_categorical_accuracy: 0.0416 - val_loss: 19.5218 - val_sparse_categorical_accuracy: 0.0354\n",
      "\n",
      "Epoch 00005: val_loss improved from 20.23747 to 19.52184, saving model to Bert_123.h5\n",
      "Epoch 6/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 14.8201 - sparse_categorical_accuracy: 0.0633 - val_loss: 18.8919 - val_sparse_categorical_accuracy: 0.0496\n",
      "\n",
      "Epoch 00006: val_loss improved from 19.52184 to 18.89186, saving model to Bert_123.h5\n",
      "Epoch 7/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 13.0584 - sparse_categorical_accuracy: 0.0934 - val_loss: 18.3488 - val_sparse_categorical_accuracy: 0.0624\n",
      "\n",
      "Epoch 00007: val_loss improved from 18.89186 to 18.34882, saving model to Bert_123.h5\n",
      "Epoch 8/25\n",
      "718/718 [==============================] - 687s 957ms/step - loss: 11.5191 - sparse_categorical_accuracy: 0.1322 - val_loss: 17.9287 - val_sparse_categorical_accuracy: 0.0733\n",
      "\n",
      "Epoch 00008: val_loss improved from 18.34882 to 17.92867, saving model to Bert_123.h5\n",
      "Epoch 9/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 10.1766 - sparse_categorical_accuracy: 0.1712 - val_loss: 17.5148 - val_sparse_categorical_accuracy: 0.0851\n",
      "\n",
      "Epoch 00009: val_loss improved from 17.92867 to 17.51477, saving model to Bert_123.h5\n",
      "Epoch 10/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 8.9192 - sparse_categorical_accuracy: 0.2177 - val_loss: 17.1619 - val_sparse_categorical_accuracy: 0.0957\n",
      "\n",
      "Epoch 00010: val_loss improved from 17.51477 to 17.16188, saving model to Bert_123.h5\n",
      "Epoch 11/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 7.6992 - sparse_categorical_accuracy: 0.2860 - val_loss: 16.8878 - val_sparse_categorical_accuracy: 0.1042\n",
      "\n",
      "Epoch 00011: val_loss improved from 17.16188 to 16.88778, saving model to Bert_123.h5\n",
      "Epoch 12/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 6.6266 - sparse_categorical_accuracy: 0.3566 - val_loss: 16.5800 - val_sparse_categorical_accuracy: 0.1135\n",
      "\n",
      "Epoch 00012: val_loss improved from 16.88778 to 16.57998, saving model to Bert_123.h5\n",
      "Epoch 13/25\n",
      "718/718 [==============================] - 686s 955ms/step - loss: 5.6250 - sparse_categorical_accuracy: 0.4278 - val_loss: 16.3540 - val_sparse_categorical_accuracy: 0.1191\n",
      "\n",
      "Epoch 00013: val_loss improved from 16.57998 to 16.35401, saving model to Bert_123.h5\n",
      "Epoch 14/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 4.7922 - sparse_categorical_accuracy: 0.5021 - val_loss: 16.1479 - val_sparse_categorical_accuracy: 0.1278\n",
      "\n",
      "Epoch 00014: val_loss improved from 16.35401 to 16.14787, saving model to Bert_123.h5\n",
      "Epoch 15/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 4.0763 - sparse_categorical_accuracy: 0.5776 - val_loss: 15.9691 - val_sparse_categorical_accuracy: 0.1355\n",
      "\n",
      "Epoch 00015: val_loss improved from 16.14787 to 15.96911, saving model to Bert_123.h5\n",
      "Epoch 16/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 3.4109 - sparse_categorical_accuracy: 0.6728 - val_loss: 15.7750 - val_sparse_categorical_accuracy: 0.1443\n",
      "\n",
      "Epoch 00016: val_loss improved from 15.96911 to 15.77499, saving model to Bert_123.h5\n",
      "Epoch 17/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 2.7647 - sparse_categorical_accuracy: 0.7803 - val_loss: 15.6073 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00017: val_loss improved from 15.77499 to 15.60729, saving model to Bert_123.h5\n",
      "Epoch 18/25\n",
      "718/718 [==============================] - 686s 955ms/step - loss: 2.3498 - sparse_categorical_accuracy: 0.8545 - val_loss: 15.4610 - val_sparse_categorical_accuracy: 0.1602\n",
      "\n",
      "Epoch 00018: val_loss improved from 15.60729 to 15.46102, saving model to Bert_123.h5\n",
      "Epoch 19/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 1.8459 - sparse_categorical_accuracy: 0.9216 - val_loss: 15.3172 - val_sparse_categorical_accuracy: 0.1651\n",
      "\n",
      "Epoch 00019: val_loss improved from 15.46102 to 15.31723, saving model to Bert_123.h5\n",
      "Epoch 20/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 1.4690 - sparse_categorical_accuracy: 0.9574 - val_loss: 15.2151 - val_sparse_categorical_accuracy: 0.1677\n",
      "\n",
      "Epoch 00020: val_loss improved from 15.31723 to 15.21506, saving model to Bert_123.h5\n",
      "Epoch 21/25\n",
      "718/718 [==============================] - 687s 956ms/step - loss: 1.1664 - sparse_categorical_accuracy: 0.9715 - val_loss: 15.0512 - val_sparse_categorical_accuracy: 0.1754\n",
      "\n",
      "Epoch 00021: val_loss improved from 15.21506 to 15.05124, saving model to Bert_123.h5\n",
      "Epoch 22/25\n",
      "718/718 [==============================] - 686s 956ms/step - loss: 0.7452 - sparse_categorical_accuracy: 0.9869 - val_loss: 14.9354 - val_sparse_categorical_accuracy: 0.1799\n",
      "\n",
      "Epoch 00022: val_loss improved from 15.05124 to 14.93536, saving model to Bert_123.h5\n",
      "Epoch 23/25\n",
      "718/718 [==============================] - 687s 957ms/step - loss: 0.5239 - sparse_categorical_accuracy: 0.9908 - val_loss: 14.8934 - val_sparse_categorical_accuracy: 0.1825\n",
      "\n",
      "Epoch 00023: val_loss improved from 14.93536 to 14.89338, saving model to Bert_123.h5\n",
      "Epoch 24/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 0.4894 - sparse_categorical_accuracy: 0.9882 - val_loss: 14.8059 - val_sparse_categorical_accuracy: 0.1849\n",
      "\n",
      "Epoch 00024: val_loss improved from 14.89338 to 14.80595, saving model to Bert_123.h5\n",
      "Epoch 25/25\n",
      "718/718 [==============================] - 688s 958ms/step - loss: 0.4090 - sparse_categorical_accuracy: 0.9905 - val_loss: 14.7422 - val_sparse_categorical_accuracy: 0.1873\n",
      "\n",
      "Epoch 00025: val_loss improved from 14.80595 to 14.74216, saving model to Bert_123.h5\n"
     ]
    }
   ],
   "source": [
    "def load_train_and_evaluate(x_train, x_val, y_train, y_val):\n",
    "    seed_everything(SEED)\n",
    "    # Load BERT from the Tensorflow Hub\n",
    "    module_url = \"../input/bert-en-uncased-l24-h1024-a16-1\"\n",
    "    bert_layer = hub.KerasLayer(module_url, trainable = True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n",
    "    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n",
    "    y_train = y_train.values\n",
    "    y_val = y_val.values\n",
    "    # Add targets to train and val\n",
    "    x_train = (x_train[0], x_train[1], x_train[2],y_train)\n",
    "    x_val = (x_val[0], x_val[1], x_val[2],y_val)\n",
    "    bert_model = build_bert_model(bert_layer, max_len = 70)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Bert_{SEED}.h5', \n",
    "                                                    monitor = 'val_loss', \n",
    "                                                    verbose = VERBOSE, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    history = bert_model.fit(x_train, y_train,\n",
    "                             validation_data = (x_val, y_val),\n",
    "                             epochs = EPOCHS, \n",
    "                             callbacks = [checkpoint],\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             verbose = VERBOSE)\n",
    "    \n",
    "    \n",
    "\n",
    "df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n",
    "load_train_and_evaluate(x_train, x_val, y_train, y_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17406.777084,
   "end_time": "2021-05-08T23:27:25.401168",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-08T18:37:18.624084",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
